{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "inputdata = {'x1': 0.04, 'x2': 0.20}\n",
    "\n",
    "# For the input ğ‘¥1 and ğ‘¥2(0.04 and 0.20), we need to train the neural network to find the weight ğ‘¤ğ‘–(ğ‘– = 1,2â€¦6)\n",
    "# 1) Generate initial random weight between (0,1);  \n",
    "\n",
    "\n",
    "# Output data\n",
    "outputdata = {'x1': 0.50}\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â€¢ Two nodes in the hidden layer: â„1, â„2 \n",
    "\n",
    "â€¢ Initial random weight: ğ‘¤1, ğ‘¤2, ğ‘¤3, ğ‘¤4, ğ‘¤5, ğ‘¤6 \n",
    "\n",
    "â€¢ Learning rate: ğ›¼ =0.4 \n",
    "\n",
    " The training process in the first round mainly includes: \n",
    "\n",
    "2) In the forward propagation, calculate the output of the node in the output layer \n",
    "3) Calculate the error function \n",
    "4) In the back propagation, use gradient descent to update all weights "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
