{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "x1 = 0.04\n",
    "x2 = 0.20\n",
    "\n",
    "# Output data\n",
    "y = 0.50\n",
    "\n",
    "# For the input ùë•1 and ùë•2(0.04 and 0.20), we need to train the neural network to find the weight ùë§ùëñ(ùëñ = 1,2‚Ä¶6)\n",
    "# 1) Generate initial random weight between (0,1);  \n",
    "w1 = np.random.rand()\n",
    "w2 = np.random.rand()\n",
    "w3 = np.random.rand()\n",
    "w4 = np.random.rand()\n",
    "w5 = np.random.rand()\n",
    "w6 = np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1: 0.525959776815054\n",
      "h2: 0.5204988329371637\n"
     ]
    }
   ],
   "source": [
    "# 2) In the forward propagation, calculate the output of the node in the output layer \n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + exp(-x))\n",
    "\n",
    "h1 = w1 * x1 + w2 * x2\n",
    "h2 = w3 * x1 + w4 * x2\n",
    "\n",
    "out_h1 = sigmoid(h1)\n",
    "out_h2 = sigmoid(h2)\n",
    "\n",
    "print('h1:', out_h1)\n",
    "print('h2:', out_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1: 0.000336955006143708\n",
      "E2: 0.00021010107589287372\n",
      "E_total: 0.0005470560820365817\n"
     ]
    }
   ],
   "source": [
    "# 3) Calculate the error function \n",
    "E_out1 = ((y - out_h1)**2) / 2\n",
    "E_out2 = ((y - out_h2)**2) / 2\n",
    "\n",
    "E_total = E_out1 + E_out2\n",
    "\n",
    "print('E1:', E_out1)\n",
    "print('E2:', E_out2)\n",
    "print('E_total:', E_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) In the back propagation, use gradient descent to update all weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚Ä¢ Two nodes in the hidden layer: ‚Ñé1, ‚Ñé2 \n",
    "‚Ä¢ Learning rate: ùõº =0.4 \n",
    "\n",
    "Training in a neural network mainly includes forward propagation and back propagation. \n",
    "In this neural network, understanding the principle to update the weights is very important.  \n",
    "\n",
    "In the back propagation, please write all equations to calculate/update the weights via gradient descent.  \n",
    "Please calculate the error with the update weights, and then compare with the error when we use the initial random weights. \n",
    "\n",
    "Programming the stopping condition: the difference between the error in a round and the error in the previous round is lower than threshold. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
